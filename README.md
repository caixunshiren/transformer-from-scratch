# transformer-from-scratch
Re-implementing the transformer mechanism from scratch (practice purpose)

Based on the famous paper: Attention Is All You Need

Done:
1. attention architecture
2. transformer architecture
3. position encoding

TODO:
1. training code
2. zero paddings for batched training
3. initial embeddings
